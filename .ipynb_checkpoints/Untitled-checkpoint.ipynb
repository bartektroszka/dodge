{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Lets try logistic regression\n",
    "#some rows are 0,0,0,0,0, we dont need them\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "base_df = pd.read_csv('./data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        #(out)input_dim is size of our (out)input data\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred_y = torch.sigmoid(self.linear(x))\n",
    "        return torch.squeeze(pred_y, 1)\n",
    "\n",
    "def LR_train(model, x, y, epochs=100, learning_rate=0.01, weight_decay=1e-5, verbose=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    loss = nn.BCELoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # From numpy to PyTorch tensors.\n",
    "        # .to(dev) puts code on either gpu or cpu.\n",
    "        X = torch.FloatTensor(x).to(dev)\n",
    "        Y = torch.FloatTensor(y).to(dev)\n",
    "        \n",
    "        # calls forward method of the model\n",
    "        y_hat = model(X)\n",
    "        # Using mean squared errors loss function\n",
    "        loss_obj = loss(y_hat, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss_obj.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and epoch % 10 == 0: \n",
    "            print(loss.item())\n",
    "            \n",
    "def LR_test(model, x, y, verbose=False):\n",
    "    loss = nn.BCELoss()\n",
    "    model.eval()\n",
    "    # .to(dev) puts code on either gpu or cpu.\n",
    "    X = torch.FloatTensor(x).to(dev)\n",
    "    Y = torch.FloatTensor(y).to(dev)\n",
    "    y_hat = model(X)\n",
    "    # Using mean squared errors loss function\n",
    "    loss_obj = loss(y_hat, Y)\n",
    "    if verbose:\n",
    "        print('test loss %.3f ' % loss.item())\n",
    "    return loss_obj.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regression_df_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-32c2e07281be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#simple division into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregression_df_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtrain_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregression_df_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmsk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregression_df_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmsk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regression_df_1' is not defined"
     ]
    }
   ],
   "source": [
    "epochs_poss = list(range(100, 510, 10))\n",
    "weight_decay_poss = [0.0,  1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "learning_rate_poss = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 0.0001, 0.001, 0.01, 0.1, 1.0] \n",
    "#testing every combination of epoch ~ weight_decay ~ learning_rate values\n",
    "def cross_valid_regression(model, train_x, train_y, test_x, test_y, verbose=False):\n",
    "    \"\"\"Function to choose the best hyperparameters for a model.\"\"\"\n",
    "    min_loss = np.inf\n",
    "    best_settings = None\n",
    "\n",
    "    for (epochs, wd, lr) in tqdm(product(epochs_poss, weight_decay_poss, learning_rate_poss)):\n",
    "        train(model, train_x, train_y, epochs=epochs, learning_rate=lr, weight_decay=wd)\n",
    "        test_loss = test(model, test_x, test_y)\n",
    "        if (test_loss < min_loss) or (test_loss == min_loss and best_settings is not None and epochs < best_settings['epochs']):\n",
    "            min_loss = test_loss\n",
    "            best_settings = {'epochs': epochs, 'weight_decay': wd, 'learning_rate_poss': lr}\n",
    "    if verbose:\n",
    "        print('min loss %.3f' % min_loss)\n",
    "        print('best settings are', best_settings)\n",
    "    return min_loss, best_settings\n",
    "\n",
    "#train_sample defines size of train set\n",
    "train_sample = 0.8\n",
    "#our output is only one number\n",
    "output_dim = 1\n",
    "\n",
    "#simple division into training and testing sets\n",
    "msk = np.random.rand(len(regression_df_1)) < train_sample\n",
    "train_set = regression_df_1[msk]\n",
    "test_set = regression_df_1[~msk]\n",
    "\n",
    "#we want to predict 'dec' basing on other attributes. We dont want 'iid' and 'pid' columns\n",
    "#to have any impact on results\n",
    "train_x = train_set.drop(['dec', 'iid', 'pid'], axis=1).values\n",
    "train_y = train_set['dec'].values \n",
    "\n",
    "test_x = test_set.drop(['dec', 'iid', 'pid'], axis=1).values\n",
    "test_y = test_set['dec'].values \n",
    "\n",
    "input_dim = train_x.shape[1]\n",
    "model = LogisticRegression(input_dim, output_dim).to(dev)\n",
    "#print(cross_valid_regression(model, train_x, train_y, test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
